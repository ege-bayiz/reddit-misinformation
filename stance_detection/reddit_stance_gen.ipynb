{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import warnings\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (LoraConfig, AutoPeftModelForCausalLM, PeftModel)\n",
    "from trl import SFTTrainer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the datasets\n",
    "Tier_1  = pd.read_pickle('./curated_datasets/Tier_1.pickle')\n",
    "Tier_2  = pd.read_pickle('./curated_datasets/Tier_2.pickle')\n",
    "# Removing any shared comments from politics (I think we can keep politics comments by the way and random sample the user to get what we need)\n",
    "Tier_1=Tier_1[Tier_1['sub']!='politics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>body</th>\n",
       "      <th>Y</th>\n",
       "      <th>YM</th>\n",
       "      <th>sub</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>Bias</th>\n",
       "      <th>Cred</th>\n",
       "      <th>submission_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>948718</th>\n",
       "      <td>948718</td>\n",
       "      <td>cyi64u7</td>\n",
       "      <td>ultimis</td>\n",
       "      <td>1451638295</td>\n",
       "      <td>t3_3yz5f1</td>\n",
       "      <td>3yz5f1</td>\n",
       "      <td>I read recently on this subject that in politi...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-01</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>t5_2qh6p</td>\n",
       "      <td>0.674677</td>\n",
       "      <td>0.384738</td>\n",
       "      <td>Donald Trump Is Smart To Remind Voters Of Clin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948757</th>\n",
       "      <td>948757</td>\n",
       "      <td>cyi9cdm</td>\n",
       "      <td>Abakala</td>\n",
       "      <td>1451654458</td>\n",
       "      <td>t3_3z0cfe</td>\n",
       "      <td>3z0cfe</td>\n",
       "      <td>This is what I've been saying for a long time....</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-01</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>t5_2qh6p</td>\n",
       "      <td>0.504332</td>\n",
       "      <td>0.588008</td>\n",
       "      <td>Krauthammer's Take: Trump Promising 'Success W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948775</th>\n",
       "      <td>948775</td>\n",
       "      <td>cyiau4n</td>\n",
       "      <td>MiyegomboBayartsogt</td>\n",
       "      <td>1451660278</td>\n",
       "      <td>t3_3yzt2a</td>\n",
       "      <td>3yzt2a</td>\n",
       "      <td>Seemingly without effort, Donald Trump's presi...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-01</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>t5_2qh6p</td>\n",
       "      <td>0.468367</td>\n",
       "      <td>0.503747</td>\n",
       "      <td>Black Power Extremist Louis Farrakhan Warns Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948826</th>\n",
       "      <td>948826</td>\n",
       "      <td>cyigkcw</td>\n",
       "      <td>VirginWizard69</td>\n",
       "      <td>1451673375</td>\n",
       "      <td>t3_3z0z09</td>\n",
       "      <td>3z0z09</td>\n",
       "      <td>I loves me some Whittle. \\n\\n</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-01</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>t5_2qh6p</td>\n",
       "      <td>0.706065</td>\n",
       "      <td>0.293663</td>\n",
       "      <td>WATCH: Bill Whittle’s last monologue of the ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948830</th>\n",
       "      <td>948830</td>\n",
       "      <td>cyih1q0</td>\n",
       "      <td>VirginWizard69</td>\n",
       "      <td>1451674231</td>\n",
       "      <td>t3_3yzaci</td>\n",
       "      <td>3yzaci</td>\n",
       "      <td>Good read.</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-01</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>t5_2qh6p</td>\n",
       "      <td>0.635442</td>\n",
       "      <td>0.446968</td>\n",
       "      <td>What You Need to Know About “The Big Short” (m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394169</th>\n",
       "      <td>394169</td>\n",
       "      <td>ecz1x66</td>\n",
       "      <td>jackgoffigen</td>\n",
       "      <td>1546293621</td>\n",
       "      <td>t3_abb6fg</td>\n",
       "      <td>abb6fg</td>\n",
       "      <td>Silly sheriff, stealing federal money is for p...</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-12</td>\n",
       "      <td>Republican</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.538336</td>\n",
       "      <td>0.557574</td>\n",
       "      <td>Alabama sherif steals $1.5 million dollars of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394267</th>\n",
       "      <td>394267</td>\n",
       "      <td>eczrshg</td>\n",
       "      <td>Stephanstewart101</td>\n",
       "      <td>1546317127</td>\n",
       "      <td>t3_abb6fg</td>\n",
       "      <td>abb6fg</td>\n",
       "      <td>Is it really stealing if you are taking it fro...</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019-01</td>\n",
       "      <td>Republican</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.538336</td>\n",
       "      <td>0.557574</td>\n",
       "      <td>Alabama sherif steals $1.5 million dollars of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394467</th>\n",
       "      <td>394467</td>\n",
       "      <td>ed1n60n</td>\n",
       "      <td>jackgoffigen</td>\n",
       "      <td>1546386807</td>\n",
       "      <td>t3_abc35u</td>\n",
       "      <td>abc35u</td>\n",
       "      <td>Would you trump hating liberals stop commentin...</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019-01</td>\n",
       "      <td>Republican</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.480137</td>\n",
       "      <td>0.589825</td>\n",
       "      <td>Republican Congressman Calls on Trump to Use O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394489</th>\n",
       "      <td>394489</td>\n",
       "      <td>ed2d2fa</td>\n",
       "      <td>wowokletstalkabit</td>\n",
       "      <td>1546411599</td>\n",
       "      <td>t3_abb6fg</td>\n",
       "      <td>abb6fg</td>\n",
       "      <td>What he did was legal..</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019-01</td>\n",
       "      <td>Republican</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.538336</td>\n",
       "      <td>0.557574</td>\n",
       "      <td>Alabama sherif steals $1.5 million dollars of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395116</th>\n",
       "      <td>395116</td>\n",
       "      <td>ed9tkem</td>\n",
       "      <td>Tampammm</td>\n",
       "      <td>1546656089</td>\n",
       "      <td>t3_aaaxc4</td>\n",
       "      <td>aaaxc4</td>\n",
       "      <td>Now we're talking!</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019-01</td>\n",
       "      <td>Republican</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.395128</td>\n",
       "      <td>0.487395</td>\n",
       "      <td>Trump Threatens To Close Southern Border, End ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171168 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Index       id               author  created_utc  parent_id link_id  \\\n",
       "948718  948718  cyi64u7              ultimis   1451638295  t3_3yz5f1  3yz5f1   \n",
       "948757  948757  cyi9cdm              Abakala   1451654458  t3_3z0cfe  3z0cfe   \n",
       "948775  948775  cyiau4n  MiyegomboBayartsogt   1451660278  t3_3yzt2a  3yzt2a   \n",
       "948826  948826  cyigkcw       VirginWizard69   1451673375  t3_3z0z09  3z0z09   \n",
       "948830  948830  cyih1q0       VirginWizard69   1451674231  t3_3yzaci  3yzaci   \n",
       "...        ...      ...                  ...          ...        ...     ...   \n",
       "394169  394169  ecz1x66         jackgoffigen   1546293621  t3_abb6fg  abb6fg   \n",
       "394267  394267  eczrshg    Stephanstewart101   1546317127  t3_abb6fg  abb6fg   \n",
       "394467  394467  ed1n60n         jackgoffigen   1546386807  t3_abc35u  abc35u   \n",
       "394489  394489  ed2d2fa    wowokletstalkabit   1546411599  t3_abb6fg  abb6fg   \n",
       "395116  395116  ed9tkem             Tampammm   1546656089  t3_aaaxc4  aaaxc4   \n",
       "\n",
       "                                                     body     Y       YM  \\\n",
       "948718  I read recently on this subject that in politi...  2016  2016-01   \n",
       "948757  This is what I've been saying for a long time....  2016  2016-01   \n",
       "948775  Seemingly without effort, Donald Trump's presi...  2016  2016-01   \n",
       "948826                      I loves me some Whittle. \\n\\n  2016  2016-01   \n",
       "948830                                        Good read.   2016  2016-01   \n",
       "...                                                   ...   ...      ...   \n",
       "394169  Silly sheriff, stealing federal money is for p...  2018  2018-12   \n",
       "394267  Is it really stealing if you are taking it fro...  2019  2019-01   \n",
       "394467  Would you trump hating liberals stop commentin...  2019  2019-01   \n",
       "394489                            What he did was legal..  2019  2019-01   \n",
       "395116                                Now we're talking!   2019  2019-01   \n",
       "\n",
       "                 sub subreddit_id      Bias      Cred  \\\n",
       "948718  Conservative     t5_2qh6p  0.674677  0.384738   \n",
       "948757  Conservative     t5_2qh6p  0.504332  0.588008   \n",
       "948775  Conservative     t5_2qh6p  0.468367  0.503747   \n",
       "948826  Conservative     t5_2qh6p  0.706065  0.293663   \n",
       "948830  Conservative     t5_2qh6p  0.635442  0.446968   \n",
       "...              ...          ...       ...       ...   \n",
       "394169    Republican          NaN -0.538336  0.557574   \n",
       "394267    Republican          NaN -0.538336  0.557574   \n",
       "394467    Republican          NaN  0.480137  0.589825   \n",
       "394489    Republican          NaN -0.538336  0.557574   \n",
       "395116    Republican          NaN  0.395128  0.487395   \n",
       "\n",
       "                                          submission_body  \n",
       "948718  Donald Trump Is Smart To Remind Voters Of Clin...  \n",
       "948757  Krauthammer's Take: Trump Promising 'Success W...  \n",
       "948775  Black Power Extremist Louis Farrakhan Warns Hi...  \n",
       "948826  WATCH: Bill Whittle’s last monologue of the ye...  \n",
       "948830  What You Need to Know About “The Big Short” (m...  \n",
       "...                                                   ...  \n",
       "394169  Alabama sherif steals $1.5 million dollars of ...  \n",
       "394267  Alabama sherif steals $1.5 million dollars of ...  \n",
       "394467  Republican Congressman Calls on Trump to Use O...  \n",
       "394489  Alabama sherif steals $1.5 million dollars of ...  \n",
       "395116  Trump Threatens To Close Southern Border, End ...  \n",
       "\n",
       "[171168 rows x 14 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some more filters on tiers if needed\n",
    "Tier_1_10plus = Tier_1['author'].value_counts()\n",
    "Tier_1_10plus = Tier_1['author'].value_counts().reset_index()\n",
    "Tier_1_10plus = Tier_1_10plus[Tier_1_10plus['count']>=10]\n",
    "Tier_1_a10plus  = Tier_1.loc[Tier_1['author'].isin(Tier_1_10plus['author'].to_list())]\n",
    "Tier_1_a10plus\n",
    "#len(Tier_1_10plus['author'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "DeferredCudaCallError",
     "evalue": "CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1702400430266/work/aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. device=1, num_gpus=\u0001\n\nCUDA call was originally invoked at:\n\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/traitlets/config/application.py\", line 1077, in launch_instance\n    app.start()\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 737, in start\n    self.io_loop.start()\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n    self._run_once()\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n    handle._run()\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 524, in dispatch_queue\n    await self.process_one()\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 513, in process_one\n    await dispatch(*args)\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 418, in dispatch_shell\n    await result\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 758, in execute_request\n    reply_content = await reply_content\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 426, in do_execute\n    res = shell.run_cell(\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n    result = self._run_cell(\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n    result = runner(coro)\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_131853/1238825697.py\", line 7, in <module>\n    import torch\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/torch/__init__.py\", line 1332, in <module>\n    _C._initExtension(manager_path())\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 244, in <module>\n    _lazy_call(_check_capability)\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 241, in _lazy_call\n    _queued_calls.append((callable, traceback.format_stack()))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/torch/cuda/__init__.py:311\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     queued_call()\n\u001b[1;32m    312\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/torch/cuda/__init__.py:180\u001b[0m, in \u001b[0;36m_check_capability\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(device_count()):\n\u001b[0;32m--> 180\u001b[0m     capability \u001b[39m=\u001b[39m get_device_capability(d)\n\u001b[1;32m    181\u001b[0m     major \u001b[39m=\u001b[39m capability[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/torch/cuda/__init__.py:435\u001b[0m, in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Gets the cuda capability of a device.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39m    tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m prop \u001b[39m=\u001b[39m get_device_properties(device)\n\u001b[1;32m    436\u001b[0m \u001b[39mreturn\u001b[39;00m prop\u001b[39m.\u001b[39mmajor, prop\u001b[39m.\u001b[39mminor\n",
      "File \u001b[0;32m~/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/torch/cuda/__init__.py:453\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid device id\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m _get_device_properties(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1702400430266/work/aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. device=1, num_gpus=\u0001",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDeferredCudaCallError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m/home/yeb96/reddit-misinformation/reddit-misinformation/stance_detection/reddit_stance_gen.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bswarmcluster2.ece.utexas.edu/home/yeb96/reddit-misinformation/reddit-misinformation/stance_detection/reddit_stance_gen.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m5\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bswarmcluster2.ece.utexas.edu/home/yeb96/reddit-misinformation/reddit-misinformation/stance_detection/reddit_stance_gen.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrunning on cuda device \u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mcurrent_device()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bswarmcluster2.ece.utexas.edu/home/yeb96/reddit-misinformation/reddit-misinformation/stance_detection/reddit_stance_gen.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m### Loading the Llama2 7b - chat model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bswarmcluster2.ece.utexas.edu/home/yeb96/reddit-misinformation/reddit-misinformation/stance_detection/reddit_stance_gen.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m base_model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNousResearch/Llama-2-7b-chat-hf\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/torch/cuda/__init__.py:769\u001b[0m, in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcurrent_device\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    768\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 769\u001b[0m     _lazy_init()\n\u001b[1;32m    770\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[0;32m~/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/torch/cuda/__init__.py:317\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    313\u001b[0m             msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    314\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCUDA call failed lazily at initialization with error: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    315\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCUDA call was originally invoked at:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(orig_traceback)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    316\u001b[0m             )\n\u001b[0;32m--> 317\u001b[0m             \u001b[39mraise\u001b[39;00m DeferredCudaCallError(msg) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     \u001b[39mdelattr\u001b[39m(_tls, \u001b[39m\"\u001b[39m\u001b[39mis_initializing\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mDeferredCudaCallError\u001b[0m: CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1702400430266/work/aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. device=1, num_gpus=\u0001\n\nCUDA call was originally invoked at:\n\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/traitlets/config/application.py\", line 1077, in launch_instance\n    app.start()\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 737, in start\n    self.io_loop.start()\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n    self._run_once()\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n    handle._run()\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 524, in dispatch_queue\n    await self.process_one()\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 513, in process_one\n    await dispatch(*args)\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 418, in dispatch_shell\n    await result\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 758, in execute_request\n    reply_content = await reply_content\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 426, in do_execute\n    res = shell.run_cell(\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n    result = self._run_cell(\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n    result = runner(coro)\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_131853/1238825697.py\", line 7, in <module>\n    import torch\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/torch/__init__.py\", line 1332, in <module>\n    _C._initExtension(manager_path())\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 244, in <module>\n    _lazy_call(_check_capability)\n  File \"/home/yeb96/reddit-misinformation/reddit-misinformation/.conda/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 241, in _lazy_call\n    _queued_calls.append((callable, traceback.format_stack()))\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "print(f'running on cuda device {torch.cuda.current_device()}')\n",
    "\n",
    "### Loading the Llama2 7b - chat model\n",
    "base_model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map='sequential'\n",
    "    #max_memory=max_memory,\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "print(base_model.hf_device_map)\n",
    "\n",
    "base_text_gen = pipeline(task=\"text-generation\", model=base_model, tokenizer=llama_tokenizer, max_new_tokens=5)\n",
    "\n",
    "query_head = \"You are a helpful, respectful, and honest assistant that detects the stance of a comment with respect to its parent. Stance detection is the process of determining whether the author of a comment is in support of or against a given parent. You are provided with:\\n post: the text you that is the root of discussion.\\n parent:  the text which the comment is a reply towards.\\n comment: text that you identify the stance from.\\n\\nYou will return the stance of the comment against the parent. Only return the stance against the parent and not the original post. Always answer from the possible options given below: \\n support: The comment has a positive or supportive attitude towards the post, either explicitly or implicitly. \\n against: The comment opposes or criticizes the post, either explicitly or implicitly. \\n none: The comment is neutral or does not have a stance towards the post. \\n unsure: It is not possible to make a decision based on the information at hand.\"\n",
    "#query = \"<SYS> query_head </SYS>\" + \"\\n\\n\" + \"post: \" + row['submission_text'] + \"\\n\" + \"parent: \" + row['body_parent'] + \"\\n\" + \"comment: \" + row['body_child'] + \"\\n\" + \"stance: \"\n",
    "#query = \"[INST] \" + query + \"[/INST]\"\n",
    "'''\n",
    "query_tail=\"\"\"post: Trump praises very smart Putin\n",
    "comment: Maybe Trump is what democrats and republicans need to heal their relationship and start acting like adults again. We have a common cause: keeping this guy from doing permanent damage to a country we love.  The extremism that has entered US politics is stopping us from actually finding any common ground. \\n\\nIt's a big MAYBE, but it's possible.\n",
    "stance:\"\"\"\n",
    "\n",
    "query_head = query_head.strip()\n",
    "\n",
    "output = base_text_gen(f\"<s>[INST] {query_head + query_tail} [/INST]\")\n",
    "print(output[0]['generated_text'])\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
